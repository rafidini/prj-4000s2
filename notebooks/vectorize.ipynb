{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorisation des noms et des descriptions d'articles (textes en anglais)\r\n",
    "Notebook basé sur:  \r\n",
    "[Getting Started with Text Vectorization](https://towardsdatascience.com/getting-started-with-text-vectorization-2f2efbec6685)  \r\n",
    "[How to Vectorize Text in DataFrames for NLP Tasks — 3 Simple Techniques](https://towardsdatascience.com/how-to-vectorize-text-in-dataframes-for-nlp-tasks-3-simple-techniques-82925a5600db)  \r\n",
    "[Multimodal deep learning to predict movie genres](https://towardsdatascience.com/multimodal-deep-learning-to-predict-movie-genres-e6855f814a8a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\lcpla\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\lcpla\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\r\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\r\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\r\n",
    "import texthero as hero\r\n",
    "from texthero import preprocessing\r\n",
    "import en_core_web_sm\r\n",
    "import time\r\n",
    "import nltk\r\n",
    "nltk.download('punkt')\r\n",
    "nltk.download('wordnet')\r\n",
    "from nltk import word_tokenize\r\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\r\n",
    "import dask.dataframe as dd\r\n",
    "from dask.diagnostics import ProgressBar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Cleaning data\n",
    "We use the Texthero library. To apply the default text cleaning script run hero.clean(pandas.Series).\n",
    "It runs the following seven functions by default when using clean():\n",
    "- fillna(s) Replace not assigned values with empty spaces.\n",
    "- lowercase(s) Lowercase all text.\n",
    "- remove_digits() Remove all blocks of digits.\n",
    "- remove\\_punctuation() Remove all string.punctuation (!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~).\n",
    "- remove_diacritics() Remove all accents from strings.\n",
    "- remove_stopwords() Remove all stop words.\n",
    "- remove_whitespace() Remove all white space between words.  \n",
    "\n",
    "Word lemmatization using NLTK. The goal of lemmatization is to reduce inflectional forms and sometimes derivationally related forms of a word to a common base form. For example, ‘Finds’ can be replaced with just ‘Find’.\n",
    "\n",
    "[Texthero documentation](https://texthero.org/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def contains_digits(w):\n",
    "    \"\"\"\n",
    "    Test de la présence de chiffres dans un mot\n",
    "\n",
    "    Args:\n",
    "        w: mot à analyser\n",
    "\n",
    "    Returns:\n",
    "        booléen indiquant la présence d'un chiffre\n",
    "    \"\"\"\n",
    "    if not w.isalpha():\n",
    "        return sum([w.find(str(i))!=-1 for i in range(10) if w.find(str(i-1))==-1])>0\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(df):\r\n",
    "    \"\"\"\r\n",
    "    Nettoyage des chaines de caractères avant vectorisation\r\n",
    "\r\n",
    "    Args:\r\n",
    "        df: dataframe contenant les chaînes de caractères à nettoyer\r\n",
    "\r\n",
    "    Returns:\r\n",
    "        dataframe avec les chaînes de caractères nettoyés\r\n",
    "    \"\"\"\r\n",
    "    start=time.time()\r\n",
    "    lemmatizer = WordNetLemmatizer()\r\n",
    "    df_clean = df.apply(lambda s: hero.clean(s))\r\n",
    "    df_clean = df_clean.apply(lambda s: s.apply(lambda t: [lemmatizer.lemmatize(w) for w in word_tokenize(t) if not contains_digits(w)]).str.join(' '))\r\n",
    "    df_clean = df_clean.apply(lambda s: s.apply(lambda t: ''.join(filter(lambda c: str.isalpha(c)|str.isspace(c), t))))\r\n",
    "    df_clean = df_clean.apply(lambda s: hero.clean(s,[preprocessing.remove_whitespace]))\r\n",
    "    print(time.strftime('Temps d\\'exécution du nettoyage: %Hh %Mmin %Ss', time.gmtime(time.time()-start)))\r\n",
    "    return df_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nb NaN:\n",
      "item_name               4\n",
      "item_caption        24184\n",
      "item_description        0\n",
      "dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>item_name</th>\n      <th>item_caption</th>\n      <th>item_description</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>sankyo aluminum shade beam standing type one s...</td>\n      <td>item manufacturer sankyo aluminum size width x...</td>\n      <td>sankyo aluminum shade beam standing type one s...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>sale sale fashion coordination thick sash belt...</td>\n      <td>increased presence thick sash belt us horse le...</td>\n      <td>sale sale fashion coordination thick sash belt...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>geta paulownia made japan woman tone nose widt...</td>\n      <td>item paulownia clog yukata half width obi yuka...</td>\n      <td>geta paulownia made japan woman tone nose widt...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>limited time yen coupon issuance shoe box widt...</td>\n      <td>product description louver shoe box width sing...</td>\n      <td>limited time yen coupon issuance shoe box widt...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>post mailbox mailbox post multi family housing...</td>\n      <td>post apartment variable push lock collective m...</td>\n      <td>post mailbox mailbox post multi family housing...</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
      "text/plain": "                                           item_name  \\\n0  sankyo aluminum shade beam standing type one s...   \n1  sale sale fashion coordination thick sash belt...   \n2  geta paulownia made japan woman tone nose widt...   \n3  limited time yen coupon issuance shoe box widt...   \n4  post mailbox mailbox post multi family housing...   \n\n                                        item_caption  \\\n0  item manufacturer sankyo aluminum size width x...   \n1  increased presence thick sash belt us horse le...   \n2  item paulownia clog yukata half width obi yuka...   \n3  product description louver shoe box width sing...   \n4  post apartment variable push lock collective m...   \n\n                                    item_description  \n0  sankyo aluminum shade beam standing type one s...  \n1  sale sale fashion coordination thick sash belt...  \n2  geta paulownia made japan woman tone nose widt...  \n3  limited time yen coupon issuance shoe box widt...  \n4  post mailbox mailbox post multi family housing...  "
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "try:\r\n",
    "    train_clean = pd.read_csv(\"data/vectors/train_clean.csv\")\r\n",
    "except:\r\n",
    "    train = pd.read_csv(\"data/traduction/train_trad.csv\")\r\n",
    "    train_clean = clean_text(train)\r\n",
    "    train_clean.to_csv(\"data/vectors/train_clean.csv\",index=False)\r\n",
    "train_clean[\"item_description\"]= (train_clean.item_name.fillna('')+\" \"+train_clean.item_caption.fillna('')).str.strip()\r\n",
    "print(\"Nb NaN:\",train_clean.isna().sum(),sep=\"\\n\")\r\n",
    "train_clean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nb NaN:\n",
      "item_name              3\n",
      "item_caption        4286\n",
      "item_description       0\n",
      "dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>item_name</th>\n      <th>item_caption</th>\n      <th>item_description</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>miraie f miraie forte au au smartphone case sm...</td>\n      <td>precaution depending arrival time material cas...</td>\n      <td>miraie f miraie forte au au smartphone case sm...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>xperia premium xperia premium docomo docomo sm...</td>\n      <td>precaution depending arrival time material cas...</td>\n      <td>xperia premium xperia premium docomo docomo sm...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>mo mono mono docomo docomo notebook type smart...</td>\n      <td>product feature seamless full scale design cal...</td>\n      <td>mo mono mono docomo docomo notebook type smart...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>xperia xz notebook type case beach hawaii expe...</td>\n      <td>compatible model xperia xz xperia sony compati...</td>\n      <td>xperia xz notebook type case beach hawaii expe...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>used comme ca du mode skirt bomb toss long len...</td>\n      <td>used comme ca du mode skirt bomb toss long len...</td>\n      <td>used comme ca du mode skirt bomb toss long len...</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
      "text/plain": "                                           item_name  \\\n0  miraie f miraie forte au au smartphone case sm...   \n1  xperia premium xperia premium docomo docomo sm...   \n2  mo mono mono docomo docomo notebook type smart...   \n3  xperia xz notebook type case beach hawaii expe...   \n4  used comme ca du mode skirt bomb toss long len...   \n\n                                        item_caption  \\\n0  precaution depending arrival time material cas...   \n1  precaution depending arrival time material cas...   \n2  product feature seamless full scale design cal...   \n3  compatible model xperia xz xperia sony compati...   \n4  used comme ca du mode skirt bomb toss long len...   \n\n                                    item_description  \n0  miraie f miraie forte au au smartphone case sm...  \n1  xperia premium xperia premium docomo docomo sm...  \n2  mo mono mono docomo docomo notebook type smart...  \n3  xperia xz notebook type case beach hawaii expe...  \n4  used comme ca du mode skirt bomb toss long len...  "
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "try:\r\n",
    "    test_clean = pd.read_csv(\"data/vectors/test_clean.csv\")\r\n",
    "except:\r\n",
    "    test = pd.read_csv(\"data/traduction/test_trad.csv\")\r\n",
    "    test_clean = clean_text(test)\r\n",
    "    test_clean.to_csv(\"data/vectors/test_clean.csv\",index=False)\r\n",
    "test_clean[\"item_description\"]= (test_clean.item_name.fillna('')+\" \"+test_clean.item_caption.fillna('')).str.strip()\r\n",
    "print(\"Nb NaN:\",test_clean.isna().sum(),sep=\"\\n\")\r\n",
    "test_clean.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Binary Term Frequency  \r\n",
    "Binary Term Frequency captures presence (1) or absence (0) of term in document. Under TfidfVectorizer, we set binary parameter equal to true so that it can show just presence (1) or absence (0) and norm parameter equal to false.  \r\n",
    "[TfidfVectorizer documentation](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temps d'exécution: 00h 00min 23s\n"
     ]
    },
    {
     "data": {
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>ab</th>\n      <th>absorbent</th>\n      <th>absorbing</th>\n      <th>absorption</th>\n      <th>ac</th>\n      <th>accent</th>\n      <th>accepted</th>\n      <th>accessory</th>\n      <th>accordion</th>\n      <th>ace</th>\n      <th>...</th>\n      <th>zero</th>\n      <th>zeta</th>\n      <th>zett</th>\n      <th>zip</th>\n      <th>zipper</th>\n      <th>zippy</th>\n      <th>zirconia</th>\n      <th>zone</th>\n      <th>zoom</th>\n      <th>zori</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>212115</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>212116</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>212117</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>212118</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>212119</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>212120 rows × 2845 columns</p>\n</div>",
      "text/plain": "        ab  absorbent  absorbing  absorption  ac  accent  accepted  accessory  \\\n0        0          0          0           0   0       0         0          0   \n1        0          0          0           0   0       0         0          0   \n2        0          0          0           0   0       0         0          0   \n3        0          0          0           0   0       0         0          0   \n4        0          0          0           0   0       0         0          0   \n...     ..        ...        ...         ...  ..     ...       ...        ...   \n212115   0          0          0           0   0       0         0          0   \n212116   0          0          0           0   0       0         0          1   \n212117   0          0          0           0   0       0         0          0   \n212118   0          0          0           0   0       0         0          0   \n212119   0          0          0           0   0       0         0          0   \n\n        accordion  ace  ...  zero  zeta  zett  zip  zipper  zippy  zirconia  \\\n0               0    0  ...     0     0     0    0       0      0         0   \n1               0    0  ...     0     0     0    0       0      0         0   \n2               0    0  ...     0     0     0    0       0      0         0   \n3               0    0  ...     0     0     0    0       0      0         0   \n4               0    0  ...     0     0     0    0       0      0         0   \n...           ...  ...  ...   ...   ...   ...  ...     ...    ...       ...   \n212115          0    0  ...     0     0     0    0       0      0         0   \n212116          0    0  ...     0     0     0    0       0      0         0   \n212117          0    0  ...     0     0     0    0       0      0         0   \n212118          0    0  ...     0     0     0    0       0      0         0   \n212119          0    0  ...     0     0     0    0       0      0         0   \n\n        zone  zoom  zori  \n0          0     0     0  \n1          0     0     0  \n2          0     0     0  \n3          0     0     0  \n4          0     0     0  \n...      ...   ...   ...  \n212115     0     0     0  \n212116     0     0     0  \n212117     0     0     0  \n212118     0     0     0  \n212119     0     0     0  \n\n[212120 rows x 2845 columns]"
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start = time.time()\r\n",
    "tv =  TfidfVectorizer(binary=True, norm=False, \r\n",
    "        use_idf=False, smooth_idf=False, \r\n",
    "        lowercase=True, stop_words=\"english\",\r\n",
    "        min_df=100,max_df=1.0,\r\n",
    "        max_features=None,ngram_range=(1,1))\r\n",
    "\r\n",
    "df = pd.DataFrame(tv.fit_transform(train_clean.item_name.fillna('').to_list()).toarray(),columns=tv.get_feature_names())\r\n",
    "df = df.astype('int')\r\n",
    "df = df.apply(lambda x: x.to_list(),axis=1)\r\n",
    "print(time.strftime('Temps d\\'exécution: %Hh %Mmin %Ss', time.gmtime(time.time()-start)))\r\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Bag of Words (BoW) Term Frequency\n",
    "Bag of Words (BoW) Term Frequency captures frequency of term in document. Under TfidfVectorizer, we set binary parameter equal to false so that it can show the actual frequency of the term and norm parameter equal to none."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temps d'exécution: 00h 00min 04s\n"
     ]
    },
    {
     "data": {
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>ab</th>\n      <th>absorbent</th>\n      <th>absorbing</th>\n      <th>absorption</th>\n      <th>ac</th>\n      <th>accent</th>\n      <th>accepted</th>\n      <th>accessory</th>\n      <th>accordion</th>\n      <th>ace</th>\n      <th>...</th>\n      <th>zero</th>\n      <th>zeta</th>\n      <th>zett</th>\n      <th>zip</th>\n      <th>zipper</th>\n      <th>zippy</th>\n      <th>zirconia</th>\n      <th>zone</th>\n      <th>zoom</th>\n      <th>zori</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>212115</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>212116</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>212117</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>212118</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>212119</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>212120 rows × 2845 columns</p>\n</div>",
      "text/plain": [
       "         ab  absorbent  absorbing  absorption   ac  accent  accepted  \\\n",
       "0       0.0        0.0        0.0         0.0  0.0     0.0       0.0   \n",
       "1       0.0        0.0        0.0         0.0  0.0     0.0       0.0   \n",
       "2       0.0        0.0        0.0         0.0  0.0     0.0       0.0   \n",
       "3       0.0        0.0        0.0         0.0  0.0     0.0       0.0   \n",
       "4       0.0        0.0        0.0         0.0  0.0     0.0       0.0   \n",
       "...     ...        ...        ...         ...  ...     ...       ...   \n",
       "212115  0.0        0.0        0.0         0.0  0.0     0.0       0.0   \n",
       "212116  0.0        0.0        0.0         0.0  0.0     0.0       0.0   \n",
       "212117  0.0        0.0        0.0         0.0  0.0     0.0       0.0   \n",
       "212118  0.0        0.0        0.0         0.0  0.0     0.0       0.0   \n",
       "212119  0.0        0.0        0.0         0.0  0.0     0.0       0.0   \n",
       "\n",
       "        accessory  accordion  ace  ...  zero  zeta  zett  zip  zipper  zippy  \\\n",
       "0             0.0        0.0  0.0  ...   0.0   0.0   0.0  0.0     0.0    0.0   \n",
       "1             0.0        0.0  0.0  ...   0.0   0.0   0.0  0.0     0.0    0.0   \n",
       "2             0.0        0.0  0.0  ...   0.0   0.0   0.0  0.0     0.0    0.0   \n",
       "3             0.0        0.0  0.0  ...   0.0   0.0   0.0  0.0     0.0    0.0   \n",
       "4             0.0        0.0  0.0  ...   0.0   0.0   0.0  0.0     0.0    0.0   \n",
       "...           ...        ...  ...  ...   ...   ...   ...  ...     ...    ...   \n",
       "212115        0.0        0.0  0.0  ...   0.0   0.0   0.0  0.0     0.0    0.0   \n",
       "212116        1.0        0.0  0.0  ...   0.0   0.0   0.0  0.0     0.0    0.0   \n",
       "212117        0.0        0.0  0.0  ...   0.0   0.0   0.0  0.0     0.0    0.0   \n",
       "212118        0.0        0.0  0.0  ...   0.0   0.0   0.0  0.0     0.0    0.0   \n",
       "212119        0.0        0.0  0.0  ...   0.0   0.0   0.0  0.0     0.0    0.0   \n",
       "\n",
       "        zirconia  zone  zoom  zori  \n",
       "0            0.0   0.0   0.0   0.0  \n",
       "1            0.0   0.0   0.0   0.0  \n",
       "2            0.0   0.0   0.0   0.0  \n",
       "3            0.0   0.0   0.0   0.0  \n",
       "4            0.0   0.0   0.0   0.0  \n",
       "...          ...   ...   ...   ...  \n",
       "212115       0.0   0.0   0.0   0.0  \n",
       "212116       0.0   0.0   0.0   0.0  \n",
       "212117       0.0   0.0   0.0   0.0  \n",
       "212118       0.0   0.0   0.0   0.0  \n",
       "212119       0.0   0.0   0.0   0.0  \n",
       "\n",
       "[212120 rows x 2845 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start = time.time()\n",
    "tv =  TfidfVectorizer(binary=False, norm=None, \n",
    "        use_idf=False, smooth_idf=False, \n",
    "        lowercase=True, stop_words=\"english\",\n",
    "        min_df=100,max_df=1.0,\n",
    "        max_features=None,ngram_range=(1,1))\n",
    "\n",
    "df = pd.DataFrame(tv.fit_transform(train_clean.item_name.to_list()).toarray(), columns = tv.get_feature_names())\n",
    "print(time.strftime('Temps d\\'exécution: %Hh %Mmin %Ss', time.gmtime(time.time()-start)))\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. (L1) Normalized Term Frequency\n",
    "(L1) Normalized Term Frequency captures normalized BoW term frequency in document. Under TfidfVectorizer, we set binary parameter equal to false so that it can show the actual frequency of the term and norm parameter equal to l1.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temps d'exécution: 00h 00min 05s\n"
     ]
    },
    {
     "data": {
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>ab</th>\n      <th>absorbent</th>\n      <th>absorbing</th>\n      <th>absorption</th>\n      <th>ac</th>\n      <th>accent</th>\n      <th>accepted</th>\n      <th>accessory</th>\n      <th>accordion</th>\n      <th>ace</th>\n      <th>...</th>\n      <th>zero</th>\n      <th>zeta</th>\n      <th>zett</th>\n      <th>zip</th>\n      <th>zipper</th>\n      <th>zippy</th>\n      <th>zirconia</th>\n      <th>zone</th>\n      <th>zoom</th>\n      <th>zori</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>212115</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>212116</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.032258</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>212117</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>212118</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>212119</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>212120 rows × 2845 columns</p>\n</div>",
      "text/plain": [
       "         ab  absorbent  absorbing  absorption   ac  accent  accepted  \\\n",
       "0       0.0        0.0        0.0         0.0  0.0     0.0       0.0   \n",
       "1       0.0        0.0        0.0         0.0  0.0     0.0       0.0   \n",
       "2       0.0        0.0        0.0         0.0  0.0     0.0       0.0   \n",
       "3       0.0        0.0        0.0         0.0  0.0     0.0       0.0   \n",
       "4       0.0        0.0        0.0         0.0  0.0     0.0       0.0   \n",
       "...     ...        ...        ...         ...  ...     ...       ...   \n",
       "212115  0.0        0.0        0.0         0.0  0.0     0.0       0.0   \n",
       "212116  0.0        0.0        0.0         0.0  0.0     0.0       0.0   \n",
       "212117  0.0        0.0        0.0         0.0  0.0     0.0       0.0   \n",
       "212118  0.0        0.0        0.0         0.0  0.0     0.0       0.0   \n",
       "212119  0.0        0.0        0.0         0.0  0.0     0.0       0.0   \n",
       "\n",
       "        accessory  accordion  ace  ...  zero  zeta  zett  zip  zipper  zippy  \\\n",
       "0        0.000000        0.0  0.0  ...   0.0   0.0   0.0  0.0     0.0    0.0   \n",
       "1        0.000000        0.0  0.0  ...   0.0   0.0   0.0  0.0     0.0    0.0   \n",
       "2        0.000000        0.0  0.0  ...   0.0   0.0   0.0  0.0     0.0    0.0   \n",
       "3        0.000000        0.0  0.0  ...   0.0   0.0   0.0  0.0     0.0    0.0   \n",
       "4        0.000000        0.0  0.0  ...   0.0   0.0   0.0  0.0     0.0    0.0   \n",
       "...           ...        ...  ...  ...   ...   ...   ...  ...     ...    ...   \n",
       "212115   0.000000        0.0  0.0  ...   0.0   0.0   0.0  0.0     0.0    0.0   \n",
       "212116   0.032258        0.0  0.0  ...   0.0   0.0   0.0  0.0     0.0    0.0   \n",
       "212117   0.000000        0.0  0.0  ...   0.0   0.0   0.0  0.0     0.0    0.0   \n",
       "212118   0.000000        0.0  0.0  ...   0.0   0.0   0.0  0.0     0.0    0.0   \n",
       "212119   0.000000        0.0  0.0  ...   0.0   0.0   0.0  0.0     0.0    0.0   \n",
       "\n",
       "        zirconia  zone  zoom  zori  \n",
       "0            0.0   0.0   0.0   0.0  \n",
       "1            0.0   0.0   0.0   0.0  \n",
       "2            0.0   0.0   0.0   0.0  \n",
       "3            0.0   0.0   0.0   0.0  \n",
       "4            0.0   0.0   0.0   0.0  \n",
       "...          ...   ...   ...   ...  \n",
       "212115       0.0   0.0   0.0   0.0  \n",
       "212116       0.0   0.0   0.0   0.0  \n",
       "212117       0.0   0.0   0.0   0.0  \n",
       "212118       0.0   0.0   0.0   0.0  \n",
       "212119       0.0   0.0   0.0   0.0  \n",
       "\n",
       "[212120 rows x 2845 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start = time.time()\n",
    "tv =  TfidfVectorizer(binary=False, norm='l1', \n",
    "        use_idf=False, smooth_idf=False, \n",
    "        lowercase=True, stop_words=\"english\",\n",
    "        min_df=100,max_df=1.0,\n",
    "        max_features=None,ngram_range=(1,1))\n",
    "\n",
    "df = pd.DataFrame(tv.fit_transform(train_clean.item_name.to_list()).toarray(), columns = tv.get_feature_names())\n",
    "print(time.strftime('Temps d\\'exécution: %Hh %Mmin %Ss', time.gmtime(time.time()-start)))\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.(L2) Normalized TF-IDF\n",
    "(L2) Normalized TFIDF (Term Frequency–Inverse Document Frequency) captures normalized TFIDF in document. The below is the formula for how to compute the TFIDF.  \n",
    "Under TfidfVectorizer, we set binary parameter equal to false so that it can show the actual frequency of the term and norm parameter equal to l2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temps d'exécution: 00h 00min 10s\n"
     ]
    },
    {
     "data": {
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>ab</th>\n      <th>absorbent</th>\n      <th>absorbing</th>\n      <th>absorption</th>\n      <th>ac</th>\n      <th>accent</th>\n      <th>accepted</th>\n      <th>accessory</th>\n      <th>accordion</th>\n      <th>ace</th>\n      <th>...</th>\n      <th>zero</th>\n      <th>zeta</th>\n      <th>zett</th>\n      <th>zip</th>\n      <th>zipper</th>\n      <th>zippy</th>\n      <th>zirconia</th>\n      <th>zone</th>\n      <th>zoom</th>\n      <th>zori</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>212115</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>212116</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.114688</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>212117</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>212118</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>212119</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>212120 rows × 2845 columns</p>\n</div>",
      "text/plain": [
       "         ab  absorbent  absorbing  absorption   ac  accent  accepted  \\\n",
       "0       0.0        0.0        0.0         0.0  0.0     0.0       0.0   \n",
       "1       0.0        0.0        0.0         0.0  0.0     0.0       0.0   \n",
       "2       0.0        0.0        0.0         0.0  0.0     0.0       0.0   \n",
       "3       0.0        0.0        0.0         0.0  0.0     0.0       0.0   \n",
       "4       0.0        0.0        0.0         0.0  0.0     0.0       0.0   \n",
       "...     ...        ...        ...         ...  ...     ...       ...   \n",
       "212115  0.0        0.0        0.0         0.0  0.0     0.0       0.0   \n",
       "212116  0.0        0.0        0.0         0.0  0.0     0.0       0.0   \n",
       "212117  0.0        0.0        0.0         0.0  0.0     0.0       0.0   \n",
       "212118  0.0        0.0        0.0         0.0  0.0     0.0       0.0   \n",
       "212119  0.0        0.0        0.0         0.0  0.0     0.0       0.0   \n",
       "\n",
       "        accessory  accordion  ace  ...  zero  zeta  zett  zip  zipper  zippy  \\\n",
       "0        0.000000        0.0  0.0  ...   0.0   0.0   0.0  0.0     0.0    0.0   \n",
       "1        0.000000        0.0  0.0  ...   0.0   0.0   0.0  0.0     0.0    0.0   \n",
       "2        0.000000        0.0  0.0  ...   0.0   0.0   0.0  0.0     0.0    0.0   \n",
       "3        0.000000        0.0  0.0  ...   0.0   0.0   0.0  0.0     0.0    0.0   \n",
       "4        0.000000        0.0  0.0  ...   0.0   0.0   0.0  0.0     0.0    0.0   \n",
       "...           ...        ...  ...  ...   ...   ...   ...  ...     ...    ...   \n",
       "212115   0.000000        0.0  0.0  ...   0.0   0.0   0.0  0.0     0.0    0.0   \n",
       "212116   0.114688        0.0  0.0  ...   0.0   0.0   0.0  0.0     0.0    0.0   \n",
       "212117   0.000000        0.0  0.0  ...   0.0   0.0   0.0  0.0     0.0    0.0   \n",
       "212118   0.000000        0.0  0.0  ...   0.0   0.0   0.0  0.0     0.0    0.0   \n",
       "212119   0.000000        0.0  0.0  ...   0.0   0.0   0.0  0.0     0.0    0.0   \n",
       "\n",
       "        zirconia  zone  zoom  zori  \n",
       "0            0.0   0.0   0.0   0.0  \n",
       "1            0.0   0.0   0.0   0.0  \n",
       "2            0.0   0.0   0.0   0.0  \n",
       "3            0.0   0.0   0.0   0.0  \n",
       "4            0.0   0.0   0.0   0.0  \n",
       "...          ...   ...   ...   ...  \n",
       "212115       0.0   0.0   0.0   0.0  \n",
       "212116       0.0   0.0   0.0   0.0  \n",
       "212117       0.0   0.0   0.0   0.0  \n",
       "212118       0.0   0.0   0.0   0.0  \n",
       "212119       0.0   0.0   0.0   0.0  \n",
       "\n",
       "[212120 rows x 2845 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start = time.time()\n",
    "tv =  TfidfVectorizer(binary=False, norm='l2', \n",
    "        use_idf=True, smooth_idf=True, \n",
    "        lowercase=True, stop_words=\"english\",\n",
    "        min_df=100,max_df=0.8,\n",
    "        max_features=None,ngram_range=(1,1))\n",
    "\n",
    "df = pd.DataFrame(tv.fit_transform(train_clean.item_name.to_list()).toarray(), columns = tv.get_feature_names())\n",
    "print(time.strftime('Temps d\\'exécution: %Hh %Mmin %Ss', time.gmtime(time.time()-start)))\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Word2Vec  \n",
    "Word2Vec provides embedded representation of words. Word2Vec starts with one representation of all words in the corpus and train a NN (with 1 hidden layer) on a very large corpus of data. Python’s spacy package provides pre-trained models we can use to see how w2v works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temps d'exécution: 00h 25min 37s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0         [0.8806767, 0.5209342, 0.22395355, -0.26502186...\n",
       "1         [1.4003121, 0.7583619, 0.3834568, -0.5043833, ...\n",
       "2         [0.96584576, 0.47329023, 0.05513578, -0.224261...\n",
       "3         [1.426747, 0.83528644, 0.34153345, -0.3079996,...\n",
       "4         [0.6590709, 0.53607327, -0.050336536, -0.42528...\n",
       "                                ...                        \n",
       "212115    [1.2364701, 0.37272543, 0.14906088, -0.2349578...\n",
       "212116    [0.934301, 0.5912743, 0.29895964, -0.28099176,...\n",
       "212117    [1.7777156, 0.73792523, 0.06944884, -0.5251924...\n",
       "212118    [0.13715902, 0.27609712, 0.23340255, -0.275412...\n",
       "212119    [1.4836605, 0.617889, -0.21495572, -0.4181979,...\n",
       "Name: item_name, Length: 212120, dtype: object"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start = time.time()\n",
    "nlp = en_core_web_sm.load()\n",
    "vectors = train.item_name.apply(lambda x: nlp(x).vector)\n",
    "print(time.strftime('Temps d\\'exécution: %Hh %Mmin %Ss', time.gmtime(time.time()-start)))\n",
    "vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Choix de la vectorisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def matrix_to_serie(m,Type=None):\r\n",
    "    \"\"\"\r\n",
    "    Conversion d'une sparse matrice en série\r\n",
    "\r\n",
    "    Args:\r\n",
    "        m: matrice à convertir\r\n",
    "        Type: type attendus en sortie\r\n",
    "\r\n",
    "    Returns:\r\n",
    "        série issue de la conversion\r\n",
    "    \"\"\"\r\n",
    "    df=pd.DataFrame(m)\r\n",
    "    try:\r\n",
    "        s = df.astype(Type).apply(lambda row: row.to_list(),axis=1)\r\n",
    "    except:\r\n",
    "        s = df.apply(lambda row: row.to_list(),axis=1)\r\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize(df,var,Type=\"binary\",NaN='',min_freq=100,max_freq=1.0,size_max=None):\r\n",
    "    \"\"\"\r\n",
    "    Vectorisation des chaînes de caractères\r\n",
    "\r\n",
    "    Args:\r\n",
    "        df: dataframe à traiter\r\n",
    "        var: liste des variable(s) à vectoriser\r\n",
    "        Type: modalité de calcul des vecteurs\r\n",
    "        NaN: format des données manquantes\r\n",
    "        min_freq: féquence minimale des mots utilisés dans le vocabulaire\r\n",
    "        max_freq: féquence maximale des mots utilisés dans le vocabulaire\r\n",
    "        size_max: taille maximale des vecteurs\r\n",
    "\r\n",
    "    Returns:\r\n",
    "        dataframe avec les colonnes var vectorisées\r\n",
    "    \"\"\"\r\n",
    "    start = time.time()\r\n",
    "    vectors = df.copy()\r\n",
    "    if Type==\"binary\":\r\n",
    "        tv = TfidfVectorizer(binary=True, norm=None, \r\n",
    "            use_idf=False, smooth_idf=False, \r\n",
    "            stop_words=\"english\", max_features=size_max,\r\n",
    "            min_df=min_freq,max_df=max_freq)\r\n",
    "        for col in var:\r\n",
    "            vectors[col+\"_\"+Type] = matrix_to_serie(tv.fit_transform(vectors[col].fillna(NaN).to_list()).toarray(),'int')\r\n",
    "    elif Type=='bow':\r\n",
    "        tv =  TfidfVectorizer(norm=None, \r\n",
    "                use_idf=False, smooth_idf=False, \r\n",
    "                stop_words=\"english\", max_features=size_max,\r\n",
    "                min_df=min_freq,max_df=max_feq)\r\n",
    "    elif Type=='l1':\r\n",
    "        tv =  TfidfVectorizer(norm='l1', \r\n",
    "                use_idf=False, smooth_idf=False, \r\n",
    "                stop_words=\"english\", max_features=size_max,\r\n",
    "                min_df=min_freq,max_df=max_freq)\r\n",
    "    elif Type=='l2':\r\n",
    "        tv =  TfidfVectorizer(stop_words=\"english\",max_features=size_max,\r\n",
    "                min_df=min_freq,max_df=max_freq)\r\n",
    "    elif Type=='w2v':\r\n",
    "        nlp = en_core_web_sm.load()\r\n",
    "        for col in var:\r\n",
    "            vectors[col+\"_\"+Type] = vectors[col].fillna(NaN).apply(lambda x: nlp(x).vector)\r\n",
    "    if Type not in ['w2v','binary']:\r\n",
    "        for col in var:\r\n",
    "            vectors[col+\"_\"+Type] = matrix_to_serie(tv.fit_transform(vectors[col].fillna(NaN).to_list()).toarray())\r\n",
    "    for col in vectors.columns:\r\n",
    "        if Type in col:\r\n",
    "            print(\"Taille des vecteurs de {}:\".format(col),len(vectors.loc[0,col]))\r\n",
    "    print(time.strftime('Temps d\\'exécution ({} vectorization): %Hh %Mmin %Ss'.format(Type.capitalize()), time.gmtime(time.time()-start)))\r\n",
    "    return vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Taille des vecteurs de item_description_w2v: 96\n",
      "Temps d'exécution (W2v vectorization): 01h 04min 16s\n"
     ]
    },
    {
     "data": {
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>item_name</th>\n      <th>item_caption</th>\n      <th>item_description</th>\n      <th>item_description_w2v</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>sankyo aluminum shade beam standing type one s...</td>\n      <td>item manufacturer sankyo aluminum size width x...</td>\n      <td>sankyo aluminum shade beam standing type one s...</td>\n      <td>[1.0962551, 0.41255596, 0.079079345, -0.069799...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>sale sale fashion coordination thick sash belt...</td>\n      <td>increased presence thick sash belt us horse le...</td>\n      <td>sale sale fashion coordination thick sash belt...</td>\n      <td>[1.1558363, 0.6076791, -0.017795345, -0.047616...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>geta paulownia made japan woman tone nose widt...</td>\n      <td>item paulownia clog yukata half width obi yuka...</td>\n      <td>geta paulownia made japan woman tone nose widt...</td>\n      <td>[0.80632937, 0.38488936, 0.07236661, -0.058630...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>limited time yen coupon issuance shoe box widt...</td>\n      <td>product description louver shoe box width sing...</td>\n      <td>limited time yen coupon issuance shoe box widt...</td>\n      <td>[1.2116121, 0.62559044, 0.1324889, -0.06670587...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>post mailbox mailbox post multi family housing...</td>\n      <td>post apartment variable push lock collective m...</td>\n      <td>post mailbox mailbox post multi family housing...</td>\n      <td>[0.9028818, 0.563046, 0.076704845, -0.05939380...</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
      "text/plain": "                                           item_name  \\\n0  sankyo aluminum shade beam standing type one s...   \n1  sale sale fashion coordination thick sash belt...   \n2  geta paulownia made japan woman tone nose widt...   \n3  limited time yen coupon issuance shoe box widt...   \n4  post mailbox mailbox post multi family housing...   \n\n                                        item_caption  \\\n0  item manufacturer sankyo aluminum size width x...   \n1  increased presence thick sash belt us horse le...   \n2  item paulownia clog yukata half width obi yuka...   \n3  product description louver shoe box width sing...   \n4  post apartment variable push lock collective m...   \n\n                                    item_description  \\\n0  sankyo aluminum shade beam standing type one s...   \n1  sale sale fashion coordination thick sash belt...   \n2  geta paulownia made japan woman tone nose widt...   \n3  limited time yen coupon issuance shoe box widt...   \n4  post mailbox mailbox post multi family housing...   \n\n                                item_description_w2v  \n0  [1.0962551, 0.41255596, 0.079079345, -0.069799...  \n1  [1.1558363, 0.6076791, -0.017795345, -0.047616...  \n2  [0.80632937, 0.38488936, 0.07236661, -0.058630...  \n3  [1.2116121, 0.62559044, 0.1324889, -0.06670587...  \n4  [0.9028818, 0.563046, 0.076704845, -0.05939380...  "
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_clean2 = vectorize(train_clean,[\"item_description\"],Type=\"w2v\")\r\n",
    "train_clean2.drop('item_description',axis='columns').to_csv(\"data/vectors/train_clean.csv\",index=False)\r\n",
    "train_clean2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Taille des vecteurs de item_description_w2v: 96\n",
      "Temps d'exécution (W2v vectorization): 00h 14min 01s\n"
     ]
    },
    {
     "data": {
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>item_name</th>\n      <th>item_caption</th>\n      <th>item_description</th>\n      <th>item_description_w2v</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>miraie f miraie forte au au smartphone case sm...</td>\n      <td>precaution depending arrival time material cas...</td>\n      <td>miraie f miraie forte au au smartphone case sm...</td>\n      <td>[0.7333057, 0.17599767, 0.24099274, -0.0315121...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>xperia premium xperia premium docomo docomo sm...</td>\n      <td>precaution depending arrival time material cas...</td>\n      <td>xperia premium xperia premium docomo docomo sm...</td>\n      <td>[0.75016546, 0.19974689, 0.2601111, -0.0260416...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>mo mono mono docomo docomo notebook type smart...</td>\n      <td>product feature seamless full scale design cal...</td>\n      <td>mo mono mono docomo docomo notebook type smart...</td>\n      <td>[0.7519761, 0.2823713, 0.17091945, -0.05869386...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>xperia xz notebook type case beach hawaii expe...</td>\n      <td>compatible model xperia xz xperia sony compati...</td>\n      <td>xperia xz notebook type case beach hawaii expe...</td>\n      <td>[1.1669095, 0.5273239, 0.2156976, -0.08899468,...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>used comme ca du mode skirt bomb toss long len...</td>\n      <td>used comme ca du mode skirt bomb toss long len...</td>\n      <td>used comme ca du mode skirt bomb toss long len...</td>\n      <td>[0.8762712, 0.45833683, 0.18252678, -0.0383652...</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
      "text/plain": "                                           item_name  \\\n0  miraie f miraie forte au au smartphone case sm...   \n1  xperia premium xperia premium docomo docomo sm...   \n2  mo mono mono docomo docomo notebook type smart...   \n3  xperia xz notebook type case beach hawaii expe...   \n4  used comme ca du mode skirt bomb toss long len...   \n\n                                        item_caption  \\\n0  precaution depending arrival time material cas...   \n1  precaution depending arrival time material cas...   \n2  product feature seamless full scale design cal...   \n3  compatible model xperia xz xperia sony compati...   \n4  used comme ca du mode skirt bomb toss long len...   \n\n                                    item_description  \\\n0  miraie f miraie forte au au smartphone case sm...   \n1  xperia premium xperia premium docomo docomo sm...   \n2  mo mono mono docomo docomo notebook type smart...   \n3  xperia xz notebook type case beach hawaii expe...   \n4  used comme ca du mode skirt bomb toss long len...   \n\n                                item_description_w2v  \n0  [0.7333057, 0.17599767, 0.24099274, -0.0315121...  \n1  [0.75016546, 0.19974689, 0.2601111, -0.0260416...  \n2  [0.7519761, 0.2823713, 0.17091945, -0.05869386...  \n3  [1.1669095, 0.5273239, 0.2156976, -0.08899468,...  \n4  [0.8762712, 0.45833683, 0.18252678, -0.0383652...  "
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_clean2 = vectorize(test_clean,[\"item_description\"],Type=\"w2v\")\r\n",
    "test_clean2.drop('item_description',axis='columns').to_csv(\"data/vectors/test_clean.csv\",index=False)\r\n",
    "test_clean2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Encodage des labels (y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "array([[0, 1, 0, ..., 0, 0, 0],\n       [0, 1, 0, ..., 0, 0, 0],\n       [0, 1, 0, ..., 0, 1, 0],\n       ...,\n       [1, 0, 0, ..., 0, 0, 0],\n       [0, 0, 0, ..., 0, 1, 0],\n       [0, 0, 1, ..., 0, 0, 0]])"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels = pd.read_csv(\"data/y_train.csv\")\r\n",
    "train_labels.color_tags = train_labels.color_tags.str.strip(\"[]'\").str.split(\"', '\")\r\n",
    "mlb = MultiLabelBinarizer()\r\n",
    "mlb.fit(train_labels.color_tags.tolist())\r\n",
    "mlb.classes_\r\n",
    "\r\n",
    "# Output - array(['Beige', 'Black', 'Blue', 'Brown', 'Burgundy', 'Gold', 'Green',\r\n",
    "#      'Grey', 'Khaki', 'Multiple Colors', 'Navy', 'Orange', 'Pink',\r\n",
    "#      'Purple', 'Red', 'Silver', 'Transparent', 'White', 'Yellow'],\r\n",
    "#      dtype=object)\r\n",
    "\r\n",
    "train_labels = mlb.transform(train_labels['color_tags'].to_list())\r\n",
    "train_labels"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit (conda)",
   "metadata": {
    "interpreter": {
     "hash": "6a6625be415b05d407cf199d588f8a9902615f2b5b5dfe82d913fc69993d1177"
    }
   },
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}